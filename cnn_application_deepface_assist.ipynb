{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_application_deepface_assist",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONExDoEVb2/L471Ni0YcEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skimaza/assist/blob/main/cnn_application_deepface_assist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzi6XJ3W_wUA"
      },
      "source": [
        "# AI 전략경영MBA 경영자를 위한 딥러닝 원리의 이해\n",
        "# CNN 응용 - DeepFace 얼굴 인식 라이브러리 활용\n",
        "## DeepFace 소스: https://github.com/serengil/deepface "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh5Q6MaBjxtk"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihwsv0otJTMh"
      },
      "source": [
        "## google-api 오류때문에 http library downgrade 필요  \n",
        "https://github.com/googleapis/google-api-python-client/issues/803  \n",
        "비디오 인식 단계에서 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4pJNdAbJHaG"
      },
      "source": [
        "!pip install httplib2==0.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPEWFL-9AgDN"
      },
      "source": [
        "## DeepFace 설치\n",
        "Colab에 기본으로 포함되지 않은 패키지는 shell command로 설치해야 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86mu1moKlRhd"
      },
      "source": [
        "!pip install deepface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly-5h1STkwv-"
      },
      "source": [
        "from deepface import DeepFace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Wp1gf4oFRK"
      },
      "source": [
        "## deepface 소스코드를 가상머신에 복사\n",
        "예제에 사용하는 이미지를 다운로드하기 위해 deepface 소스를 복사  \n",
        "  \n",
        "github의 코드를 복사하는 명령은 git clone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpacPBnniyA"
      },
      "source": [
        "!git clone https://github.com/serengil/deepface.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sHsyiLCZIX"
      },
      "source": [
        "## 예제에서 사용할 이미지가 다운로드되었는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPHDJNbJAqGE"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JVzsDr_oWpf"
      },
      "source": [
        "!ls deepface/tests/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q06BwuYiP7tr"
      },
      "source": [
        "# 데이터 이미지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ElHenLP64S"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaIMHAMjDMJj"
      },
      "source": [
        "### img1.jpg 이미지 보기\n",
        "deepface/tests/dataset 폴더 아래 img1.jpg 이미지 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhXDVwjP_9Y"
      },
      "source": [
        "img1 = cv2.imread('deepface/tests/dataset/img1.jpg', cv2.IMREAD_COLOR) # 이미지 읽기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7nd2vQRFWV1"
      },
      "source": [
        "img1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL2KYJu6Fo_g"
      },
      "source": [
        "type(img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnadwXj0FR_D"
      },
      "source": [
        "* 이미지는 3차원 numpy 배열로 표현된다.\n",
        "* 3개의 차원은 각각 높이, 너비, 색상을 나타낸다.\n",
        "* 색상 차원의 순서는 라이브러리마다 차이가 있을 수 있으므로 주의가 필요\n",
        "    * matplotlib에서는 RGB, OpenCV (cv2)에서는 BGR이 디폴트임\n",
        "    * 따라서 OpenCV로 읽은 데이터를 matplotlib에서 디스플레이할 때는 색상 차원의 순서를 변환해 줘야 한다 (아래 예제 참고)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj4SPHYNRXoV"
      },
      "source": [
        "### OpenCV로 읽은 것을 plt.imshow()로 display\n",
        "이번 예제에서는 plt.imshow()로 이미지 디스플레이  \n",
        "그대로 display하면 아래와 같이 보임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO_AGV5wQ6FK"
      },
      "source": [
        "plt.imshow(img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlVqWKGyT6H5"
      },
      "source": [
        "### BGR을 RGB로 변환하여 디스플레이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3181h2tsRslo"
      },
      "source": [
        "img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img1_rgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ql2cGfEL899"
      },
      "source": [
        "print(img1[2, 2, :])\n",
        "print(img1_rgb[2,2,:]) # 첫번쨰와 세번째 값이 switch 되었다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5z0u9qTUK0T"
      },
      "source": [
        "이미지 크기가 바뀐게 아니라 plt.imshow에서 작게 디스플레이한 것임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEVUwdJbUFPh"
      },
      "source": [
        "img1_rgb.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3CC2WlRE1R7"
      },
      "source": [
        "이제 img1은 필요없으므로 img1에 img1_rgb를 assign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK4JG0eEEyMF"
      },
      "source": [
        "img1 = img1_rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqW3X37CGxg6"
      },
      "source": [
        "### plt.imshow()를 써서 이미지를 디스플레이할 때 그림 크기를 키우기 위해서는 아래와 같은 방법을 쓴다.  \n",
        "(주의) 마지막에 fig 변수를 close하지 않으면 garbage로 쌓이므로 주의.  \n",
        "위의 예와 같이 plt.imshow만 사용하는 경우에는 Colab 셀의 마지막에 plt.show()를 생략해도 Colab에서 자동으로 호출해준다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34q4gPL1FpRp"
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.imshow(img1_rgb)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PUhG5ilMsv1"
      },
      "source": [
        "### 또는 아래와 같이 글로벌 default 설정을 바꿀 수도 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMPWwqyVURl3"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWhyGsb2VV3W"
      },
      "source": [
        "plt.rcParams['figure.dpi'] # dot per inch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7a0QSJpUk2x"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [12.0, 8.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHOodmGnUzmW"
      },
      "source": [
        "plt.imshow(img1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkIh8hPPSbVe"
      },
      "source": [
        "# img2도 읽어보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mkza9HCSbA6"
      },
      "source": [
        "img2 = cv2.imread('deepface/tests/dataset/img2.jpg', cv2.IMREAD_COLOR)\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj363XS3PKKa"
      },
      "source": [
        "plt.imshow(img2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwG-qRE8X48U"
      },
      "source": [
        "## 이미지를 읽어서 색상변환하는 함수를 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW_BjsU7X4OI"
      },
      "source": [
        "def read_image(file, show=True):\n",
        "    img = cv2.imread(file, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    if show:\n",
        "        plt.imshow(img)\n",
        "        plt.show() # don't forget plt.show()\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-y64ugX9Ji"
      },
      "source": [
        "img1 = read_image('deepface/tests/dataset/img1.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44EewDkXYcc7"
      },
      "source": [
        "# 기본 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cViYG9jXYUjs"
      },
      "source": [
        "import os                # 운영체제 관련 기능을 제공하는 패키지\n",
        "import glob              # 여러 파일이름을 읽어들이는 패키지\n",
        "import numpy as np\n",
        "from pathlib import Path # 파일 경로 처리를 도와주는 패키지\n",
        "from natsort import natsorted # 파일이름을 일상에서 쓰는 순서로 정렬"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzY-PDokY5CF"
      },
      "source": [
        "## glob을 이용하여 dataset 폴더의 모든 파일리스트를 읽음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8yyMffsihNv"
      },
      "source": [
        "glob.glob(\"deepface/tests/dataset/*.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzHFgzkmZJxz"
      },
      "source": [
        "파일의 순서가 예측할 수 없게 정렬되어 있음. (운영체제에 따라 달라짐)  \n",
        "모든 파일리스트를 읽은 후에 natsorted를 이용하여 파일이름 순서로 정렬  \n",
        "natsorted는 img2 보다 img11이 뒤에 온다는 것도 고려해서 정렬하는 natural order sort 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtgF7EdyYd0z"
      },
      "source": [
        "imagefiles = natsorted(glob.glob(\"deepface/tests/dataset/*.jpg\"))\n",
        "print(imagefiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuU0y0M0ZT7k"
      },
      "source": [
        "모든 이미지를 읽어들임  \n",
        "all_titles에는 경로를 제외하고 파일이름만 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VuQfY-UiDfw"
      },
      "source": [
        "all_images = []\n",
        "all_titles = []\n",
        "for file in imagefiles:\n",
        "    all_titles.append(file.split('/')[-1])\n",
        "    all_images.append(read_image(str(file), show=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMsQ-DYxjwhv"
      },
      "source": [
        "len(all_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8tMI5mEIEqP"
      },
      "source": [
        "all_images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO_TInOzZjOQ"
      },
      "source": [
        "all_titles[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbybg5fjgkdY"
      },
      "source": [
        "여러 이미지를 그리드 형식으로 디스플레이하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln68d8a2gjfQ"
      },
      "source": [
        "def show_img(img, ax=None, title=None):\n",
        "    \"\"\"Shows a single image.\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    ax.imshow(img)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n",
        "def show_img_grid(imgs, titles, size=3, clear_last=True):\n",
        "    \"\"\"Shows a grid of images.\"\"\"\n",
        "    n_images = len(imgs)\n",
        "    n_grid = int(np.ceil(n_images**.5)) # grid size containing all images. ceiling sqrt(n_images) and convert to integer\n",
        "    total_grids = n_grid * n_grid       # 이 함수는 행과 열의 길이가 같은 그리드를 구성하여 디스플레이\n",
        "    _, axs = plt.subplots(n_grid, n_grid, squeeze=False, figsize=(size * n_grid, size * n_grid)) # squeeze=False makes axs always 2D grid\n",
        "    for i, (img, title) in enumerate(zip(imgs, titles)):\n",
        "        show_img(img, axs[i // n_grid][i % n_grid], title)\n",
        "    if clear_last:\n",
        "        if total_grids > n_images: # there're empty grid in the last row\n",
        "            for i in range(n_images, total_grids):\n",
        "                column = i - n_grid*(n_grid-1)\n",
        "                axs[n_grid-1][column].axis('off')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S2UaQSFgxlI"
      },
      "source": [
        "show_img_grid(all_images, all_titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAMmCZ3SZ747"
      },
      "source": [
        "# DeepFace 기능 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU0eJNBxO4dZ"
      },
      "source": [
        "## img1과 img2가 동일인인지 확인하는 함수 verify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsuj9F8PZ0Ge"
      },
      "source": [
        "img1.jpg와 img2.jpg가 동일인인지 검증  \n",
        "처음 실행하면 이미 학습된 결과를 다운로드한 후 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl7EVUwmlD1Z"
      },
      "source": [
        "result = DeepFace.verify(\"deepface/tests/dataset/img1.jpg\", \"deepface/tests/dataset/img2.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqFd7OPwnG2D"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-V_faMJXDAc"
      },
      "source": [
        "두 이미지의 거리는 0.255, threshold를 0.4로 설정했으므로 두 이미지는 유사하다고 판단할 수 있다. 따라서 verified가 True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RphWCQKgaUP7"
      },
      "source": [
        "위 결과는 디폴트 모델인 'VGG-Face'를 사용한 것  \n",
        "다른 모델도 적용할 수 있음  \n",
        "model_name\n",
        "        'VGG-Face', \n",
        "        'OpenFace', \n",
        "        'Facenet', \n",
        "        'Facenet512',\n",
        "        'DeepFace',\n",
        "        'DeepID',\n",
        "        'Dlib',\n",
        "        'ArcFace'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8x8Cnmfosb"
      },
      "source": [
        "result = DeepFace.verify(\"deepface/tests/dataset/img1.jpg\", \"deepface/tests/dataset/img2.jpg\", model_name='Facenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eok7WQr-W5_w"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kub2MeIjXapS"
      },
      "source": [
        "## 여러 이미지 중에 유사한 이미지 찾기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9BcG1m6omNw"
      },
      "source": [
        "img1과 유사한 이미지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IacTembdorn9"
      },
      "source": [
        "df = DeepFace.find(img_path = \"deepface/tests/dataset/img1.jpg\", db_path = \"deepface/tests/dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdoV10vjsXFd"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvtnid-TourO"
      },
      "source": [
        "jolie_files = df['identity'].to_list()\n",
        "print(jolie_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2n22t9NpKV-"
      },
      "source": [
        "jolie_images = []\n",
        "jolie_titles = []\n",
        "for file in jolie_files:\n",
        "    jolie_titles.append(file.split('/')[-1])\n",
        "    jolie_images.append(read_image(str(file), show=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL_TgLQYpVhl"
      },
      "source": [
        "show_img_grid(jolie_images, jolie_titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6i9Hl2PqZbg"
      },
      "source": [
        "df_aniston = DeepFace.find(img_path = \"deepface/tests/dataset/img3.jpg\", db_path = \"deepface/tests/dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_tb0PIlqZTZ"
      },
      "source": [
        "df_aniston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uV8gPbBqZIh"
      },
      "source": [
        "aniston_files = df_aniston['identity'].to_list()\n",
        "aniston_images = []\n",
        "aniston_titles = []\n",
        "for file in aniston_files:\n",
        "    aniston_titles.append(file.split('/')[-1])\n",
        "    aniston_images.append(read_image(str(file), show=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXvt9U0uq2qJ"
      },
      "source": [
        "show_img_grid(aniston_images, aniston_titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exJX7KSKE5x2"
      },
      "source": [
        "# Facial Emotion Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTY842rauDkm"
      },
      "source": [
        "### analyze 함수 사용\n",
        "analyze 함수는 얼굴 이미지에서 얼굴 부분을 찾고 감정/나이/성별/인종 분류를 지원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLpYEy04sUKK"
      },
      "source": [
        "img8 = read_image(\"deepface/tests/dataset/img8.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UrcLIhjtGwZ"
      },
      "source": [
        "obj = DeepFace.analyze(img_path = \"deepface/tests/dataset/img8.jpg\", actions = ['age', 'gender', 'race', 'emotion'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMLJPMXQsGmt"
      },
      "source": [
        "obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx2xBwOcLgiH"
      },
      "source": [
        "obj.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtinlZU3LueC"
      },
      "source": [
        "r = obj['region']\n",
        "h, w, x, y = r['h'], r['w'], r['x'], r['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LWIffrxLvHL"
      },
      "source": [
        "r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDLGqyUrMSE_"
      },
      "source": [
        "import matplotlib.patches as patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4cCyOGpLvHN"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(img8)\n",
        "rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor='r', facecolor='none')\n",
        "ax.add_patch(rect)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7qYmwTkv2ka"
      },
      "source": [
        "# 자신의 이미지로 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfXTnYjIXakS"
      },
      "source": [
        "# image capture from webcam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWg3BFpQXZvY"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZcrDzrYXZrq"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOAI-WSAhIlg"
      },
      "source": [
        "디폴트 face detector (opencv)는 얼굴을 놓치는 경우가 많다.  \n",
        "detector는 mtcnn을 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUU_Bbz1XZkZ"
      },
      "source": [
        "myobj = DeepFace.analyze(img_path = \"photo.jpg\", actions = ['age', 'gender', 'race', 'emotion'], detector_backend='mtcnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haUDqhbAXZiW"
      },
      "source": [
        "myobj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTFLH30Yln3"
      },
      "source": [
        "myimg = cv2.imread('photo.jpg')\n",
        "myimg = cv2.cvtColor(myimg, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(myimg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFK4YvjNbcs"
      },
      "source": [
        "r = myobj['region']\n",
        "h, w, x, y = r['h'], r['w'], r['x'], r['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htH7H976Ylfl"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(myimg)\n",
        "rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor='r', facecolor='none')\n",
        "ax.add_patch(rect)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpIhoX66YldD"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))\n",
        "\n",
        "myobj = DeepFace.analyze(img_path = \"photo.jpg\", actions = ['age', 'gender', 'race', 'emotion'], detector_backend='mtcnn')\n",
        "\n",
        "myimg = cv2.imread('photo.jpg')\n",
        "myimg = cv2.cvtColor(myimg, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "r = myobj['region']\n",
        "h, w, x, y = r['h'], r['w'], r['x'], r['y']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(myimg)\n",
        "rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor='r', facecolor='none')\n",
        "ax.add_patch(rect)\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "myobj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGRjkw_fYlX1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjdSbkUoOdCB"
      },
      "source": [
        "# 모델 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1fRUMFQYlVg"
      },
      "source": [
        "vgg = DeepFace.build_model('VGG-Face')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Zwuz2HXZVj"
      },
      "source": [
        "vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz9pjuGzOlRO"
      },
      "source": [
        "## 모델 디스플레이를 위해 tensorflow import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpKP9YqCiwuY"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH-li-zDiZ2z"
      },
      "source": [
        "tf.keras.utils.plot_model(vgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sugFb6R4C7Vu"
      },
      "source": [
        "# Video recording from webcam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_3fyuS0V7dn"
      },
      "source": [
        "from IPython.display import display, Javascript,HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbUL-Y4AV7Yf"
      },
      "source": [
        "def record_video(filename):\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "       \n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        " \n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        " \n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        " \n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
        "     \n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        " \n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        " \n",
        "      await video.play();\n",
        " \n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        " \n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        " \n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "       \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        " \n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(filename,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{filename}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izyQUqLoV7WH"
      },
      "source": [
        "video_path = \"webcam1.mp4\"\n",
        "record_video(video_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_RfcRSTsyUG"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-MfaLspWypQ"
      },
      "source": [
        "## 다음 함수는 짧은 비디오에서만 동작\n",
        "길어지면 Colab에서 오류 발생  \n",
        "예제에서는 1분 이내의 비디오이므로 문제없음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zFSXZ0CV7QQ"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "def show_video(video_path, video_width = 600):\n",
        "   \n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\" type=\"video/mp4\"></video>\"\"\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A0gzGcsV7Mb"
      },
      "source": [
        "show_video(\"webcam1.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM3Iyo6ou8oE"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHGONQ-YXADM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ZGxCWqQt60"
      },
      "source": [
        "## 10 프레임 간격으로 이미지 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GaJOJoMxOUA"
      },
      "source": [
        "images = []\n",
        "cap = cv2.VideoCapture('webcam1.mp4')\n",
        "i = 0\n",
        "while True:\n",
        "    ret, image = cap.read()\n",
        "    if ret:\n",
        "        if i % 10 == 0:\n",
        "            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            images.append(img_rgb)\n",
        "        i += 1\n",
        "    else:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gJNjhblt8aJ"
      },
      "source": [
        "len(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ034n2oxsoS"
      },
      "source": [
        "plt.imshow(images[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHXbO0O9SOBx"
      },
      "source": [
        "### enforce_detection=False\n",
        "얼굴 탐지에 실패해도 오류를 내지 않고 계속 진행  \n",
        "전체 이미지를 입력으로 분석 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAUa0q3uxskJ"
      },
      "source": [
        "results = DeepFace.analyze(img_path = images, actions = ['age', 'gender', 'race', 'emotion'], detector_backend='mtcnn', enforce_detection=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Zmp2rHRy-8e"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHbIVhgkxshw"
      },
      "source": [
        "face_locations = []\n",
        "emotions = []\n",
        "for i, (inst, res) in enumerate(results.items()):\n",
        "    print(i, res['age'], res['dominant_race'], res['gender'], res['dominant_emotion'])\n",
        "    print(res['region'])\n",
        "    face_locations.append([res['region']['x'], res['region']['y'], res['region']['w'], res['region']['h']])\n",
        "    emotions.append(res['dominant_emotion'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6hjtlllzzAu"
      },
      "source": [
        "face_locations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mrpG-ha1GP3"
      },
      "source": [
        "\n",
        "def show_img2(img, rect, ax=None, title=None):\n",
        "    \"\"\"Shows a single image.\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    #ax.imshow(img[...])\n",
        "    ax.imshow(img)\n",
        "    patch = patches.Rectangle(rect[:2], rect[2], rect[3], linewidth=3, edgecolor='g', facecolor=\"none\")\n",
        "    ax.add_patch(patch)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n",
        "def show_img_grid2(imgs, rects, titles, size=3, clear_last=True):\n",
        "    \"\"\"Shows a grid of images.\"\"\"\n",
        "    n_images = len(imgs)\n",
        "    n_grid = int(np.ceil(n_images**.5)) # grid size containing all images. ceiling sqrt(n_images) and convert to integer\n",
        "    total_grids = n_grid * n_grid\n",
        "    fig, axs = plt.subplots(n_grid, n_grid, squeeze=False, figsize=(size * n_grid, size * n_grid)) # squeeze=False makes axs always 2D grid\n",
        "    for i, (img, rect, title) in enumerate(zip(imgs, rects, titles)):\n",
        "        show_img2(img, rect, axs[i // n_grid][i % n_grid], title)\n",
        "    if clear_last:\n",
        "        if total_grids > n_images: # there're empty grid in the last row\n",
        "            for i in range(n_images, total_grids):\n",
        "                column = i - n_grid*(n_grid-1)\n",
        "                axs[n_grid-1][column].axis('off')\n",
        "    plt.show()\n",
        "    plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PonmfrL18172"
      },
      "source": [
        "def show_image_table2(imgs, boxes, titles=None, rows=3, cols=4,  figsize=(15,15)):\n",
        "    total_pictures = rows * cols\n",
        "    #boxes = [[b['x'], b['y'], b[]]]\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, squeeze=None, figsize=figsize)\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            ind = i*cols + j\n",
        "            #img = plt.imread(str(files[ind]))\n",
        "            img = imgs[ind]\n",
        "            axes[i,j].imshow(img)\n",
        "            if boxes is not None:\n",
        "                box = boxes[ind]\n",
        "                rect = patches.Rectangle(box[:2], box[2], box[3], linewidth=3, edgecolor='g', facecolor='none')\n",
        "                axes[i,j].add_patch(rect)\n",
        "            axes[i,j].axis('off')\n",
        "            if titles:\n",
        "                axes[i,j].set_title(titles[ind])\n",
        "            else:\n",
        "                axes[i,j].set_title(str(ind))\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3VsVOyd-YEp"
      },
      "source": [
        "show_image_table2(images, face_locations, titles=emotions) #, size=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcrfW8FC1GID"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXYC-8gIW0XF"
      },
      "source": [
        "## Face detector를 Retina face로 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7lWghm71GBV"
      },
      "source": [
        "results = DeepFace.analyze(img_path = images, actions = ['age', 'gender', 'race', 'emotion'], detector_backend='retinaface', enforce_detection=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxxa0Zgc1F-5"
      },
      "source": [
        "face_locations = []\n",
        "emotions = []\n",
        "for i, (inst, res) in enumerate(results.items()):\n",
        "    #print(i, res['age'], res['dominant_race'], res['gender'], res['dominant_emotion'])\n",
        "    #print(res['region'])\n",
        "    face_locations.append([res['region']['x'], res['region']['y'], res['region']['w'], res['region']['h']])\n",
        "    emotions.append(res['dominant_emotion'])\n",
        "\n",
        "show_image_table2(images, face_locations, titles=emotions) #, size=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjPw9eVE1F7J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}